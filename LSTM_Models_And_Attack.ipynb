{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Models_And_Attack.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPu4yXd6X2WoWEixFFy8v4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misha345a/E-commerce_Reviews_Classifier/blob/main/LSTM_Models_And_Attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal\n",
        "Train a deep learning model to predict Recommended vs Not Recommended classification based on customer reviews. <br> Then, attack the trained models using TextAttack to evaluate model robustness."
      ],
      "metadata": {
        "id": "a6EgaGuTa7yY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9eJUaxBa1bb"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "\n",
        "!pip install openpyxl -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "v5ZCnoVBG4rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "# initiate tqdm for pandas.apply() functions\n",
        "tqdm_notebook.pandas()"
      ],
      "metadata": {
        "id": "rXTD4wa4a35E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expand notebook display options for dataframes\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "pd.options.display.max_columns = 999\n",
        "pd.options.display.max_rows = 300"
      ],
      "metadata": {
        "id": "VYJ-cNjfa38C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset of your choice\n",
        "# dataset = pd.read_excel('/content/Raw_Dataset_(Cleaned).xlsx')\n",
        "dataset = pd.read_excel('/content/Upsampled_Dataset.xlsx')\n",
        "# dataset = pd.read_excel('/content/Augmented_Dataset.xlsx')"
      ],
      "metadata": {
        "id": "7G68DN8fe0y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check value counts of prediction class\n",
        "dataset['Recommended IND'].value_counts()"
      ],
      "metadata": {
        "id": "P3Fkgn1nyYG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a326ec5-b744-4f59-bbc1-20f68c35ebed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    18540\n",
              "0    16104\n",
              "Name: Recommended IND, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a random seed for reproducible results\n",
        "random_state = 42\n",
        "\n",
        "# shuffle the dataset rows\n",
        "dataset = dataset.sample(frac=1,\n",
        "                         random_state=random_state)"
      ],
      "metadata": {
        "id": "FKQs20uXNGTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset.drop('Recommended IND', axis=1)\n",
        "y = dataset['Recommended IND']\n",
        "\n",
        "# split the dataset into an 80% training and 20% test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=random_state,\n",
        "                                                    shuffle=True)"
      ],
      "metadata": {
        "id": "IhM155nwyA9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "_MX0SLX_FAdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "rp-MZMJjyYKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilize the most frequently apprearing words in the corpus\n",
        "num_words = 10000\n",
        "\n",
        "# tokenize the training data\n",
        "tokenizer = Tokenizer(num_words=num_words,\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n1234567890') \n",
        "corpus = X_train['Review Text'].tolist() + X_test['Review Text'].tolist()\n",
        "tokenizer.fit_on_texts(corpus)"
      ],
      "metadata": {
        "id": "B66t-VE2yA_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the data word index\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "c3ehgPGI_3A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode training/test data into sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train['Review Text'].tolist())\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test['Review Text'].tolist())"
      ],
      "metadata": {
        "id": "YMaiWv6dyBPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the max number of words to consider in each review\n",
        "maxlen = max([len(x) for x in X_train_seq])\n",
        "print(f\"Max sequence length: {maxlen}\\n\")\n",
        "\n",
        "# truncate and pad the training/test input sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# output the resulting dimensions \n",
        "print(\"Padded shape (training):\".ljust(25), X_train_pad.shape)\n",
        "print(\"Padded shape (test):\".ljust(25), X_test_pad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-TwX56-yBRh",
        "outputId": "67395690-6b4b-4a1d-8a7b-7f1228308c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 117\n",
            "\n",
            "Padded shape (training):  (27715, 117)\n",
            "Padded shape (test):      (6929, 117)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Neural Network"
      ],
      "metadata": {
        "id": "OWSuEhrBQcbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate LSTM for sequence classification\n",
        "model = Sequential()\n",
        "\n",
        "# embed each numeric in a 50-dimensional vector\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                    50,     \n",
        "                    input_length=maxlen))\n",
        "\n",
        "# add bidirectional LSTM layer\n",
        "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "# add a classifier \n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "    \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LD6vyNv88e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2451f7-1a46-4b5c-e551-cd94db17bf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 117, 50)           545850    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               60400     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 606,351\n",
            "Trainable params: 606,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "num_epochs = 5\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train_pad, y_train, \n",
        "          epochs=num_epochs,\n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ov4ddKqc8e_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0c7a6b-ce36-462c-a92b-203ade1dff81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "55/55 [==============================] - 97s 2s/step - loss: 0.5579 - accuracy: 0.7068\n",
            "Epoch 2/5\n",
            "55/55 [==============================] - 92s 2s/step - loss: 0.3059 - accuracy: 0.8751\n",
            "Epoch 3/5\n",
            "55/55 [==============================] - 91s 2s/step - loss: 0.2324 - accuracy: 0.9134\n",
            "Epoch 4/5\n",
            "55/55 [==============================] - 95s 2s/step - loss: 0.2003 - accuracy: 0.9273\n",
            "Epoch 5/5\n",
            "55/55 [==============================] - 90s 2s/step - loss: 0.1782 - accuracy: 0.9381\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b16bad110>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "PDXOMqatJw0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on the test set\n",
        "model.evaluate(X_test_pad, y_test)\n",
        "y_test_pred = (model.predict(X_test_pad) >= 0.5).astype(\"int32\") "
      ],
      "metadata": {
        "id": "fvIbh4Rc8fEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a074b2-13b9-4b8a-c939-0ed71f0d6b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "217/217 [==============================] - 9s 37ms/step - loss: 0.2080 - accuracy: 0.9265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdZ12207CAch",
        "outputId": "13823ca9-7f19-45f9-9516-963c977d06a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92      3222\n",
            "           1       0.97      0.89      0.93      3707\n",
            "\n",
            "    accuracy                           0.93      6929\n",
            "   macro avg       0.93      0.93      0.93      6929\n",
            "weighted avg       0.93      0.93      0.93      6929\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model (optional)"
      ],
      "metadata": {
        "id": "KKCf_ZK0J71R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the entire model\n",
        "model.save('LSTM_Raw_Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T8ThCvmV-2t",
        "outputId": "b1fb04e2-dec1-4ef7-c0fe-9bc7f0ce4524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM_Raw_Dataset/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: LSTM_Raw_Dataset/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b8e244250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model (optional)"
      ],
      "metadata": {
        "id": "7sdBk6RE2rZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reload a fresh Keras model from the saved model\n",
        "new_model = tf.keras.models.load_model('/content/LSTM_Raw_Dataset')"
      ],
      "metadata": {
        "id": "lX6KTiT6HgtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the maxlen variable of the model\n",
        "model_config = new_model.get_config()\n",
        "maxlen = model_config['layers'][0]['config']['batch_input_shape'][1]\n",
        "print(maxlen)"
      ],
      "metadata": {
        "id": "SmRacjn0GF6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Predictions (Examples)"
      ],
      "metadata": {
        "id": "LfT0myn_KVou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 randomely selected reviews\n",
        "reviews = [\"this dress is perfection! so pretty and flattering.\",\n",
        "           \"this is my new favorite top! looks and fits as described.\",\n",
        "           \"i could wear this every day, it is stylish and comfortable\",\n",
        "           \"material is too thin and quality is poor\",\n",
        "           \"it is nice material but the design makes you look like a pregnant lady\"]"
      ],
      "metadata": {
        "id": "rz-gOlRgKl89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pred(text):\n",
        "  \"\"\"\n",
        "  Use the trained LSTM to make predictions on new examples.\n",
        "  \"\"\"\n",
        "  tokens = tokenizer.texts_to_sequences([text]) \n",
        "  tokens_pad = pad_sequences(tokens, maxlen=maxlen)\n",
        "  tokens_pad.shape\n",
        "  model_pred = model.predict(tokens_pad)\n",
        "\n",
        "  conf_val = model_pred[0][0]\n",
        "  if conf_val>=0.5:\n",
        "    print( f\"'{text}'\\nRecommended | {int(conf_val*100)}% Confidence\\n\")\n",
        "  else:\n",
        "    print( f\"'{text}'\\nNot Recommended | {int(conf_val*100)}% Confidence\\n\")  "
      ],
      "metadata": {
        "id": "9I4235yWWUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in reviews:\n",
        "  model_pred(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64zzy5lX84PC",
        "outputId": "ebb20c4c-e21d-4c29-c137-7744e2961d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'this dress is perfection! so pretty and flattering.'\n",
            "Recommended | 93% Confidence\n",
            "\n",
            "'this is my new favorite top! looks and fits as described.'\n",
            "Recommended | 75% Confidence\n",
            "\n",
            "'i could wear this every day, it is stylish and comfortable'\n",
            "Recommended | 95% Confidence\n",
            "\n",
            "'material is too thin and quality is poor'\n",
            "Not Recommended | 5% Confidence\n",
            "\n",
            "'it is nice material but the design makes you look like a pregnant lady'\n",
            "Not Recommended | 11% Confidence\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Textual Adversarial Attack"
      ],
      "metadata": {
        "id": "oxcqNo0vPRVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install textattack\n",
        "!pip install tensorflow_text "
      ],
      "metadata": {
        "id": "HEwasoMs5_7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.models.wrappers import ModelWrapper\n",
        "from textattack import AttackArgs\n",
        "from textattack.datasets import Dataset\n",
        "from textattack import Attacker\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66CB2rDm5_9N",
        "outputId": "cf679d04-0c35-4c16-926c-b89b9e03bd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:12<00:00, 39.9MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmp9q9l_kwf.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorFlowModelWrapper(ModelWrapper):\n",
        "  \"\"\"\n",
        "  Implementation of a model wrapper class to\n",
        "  run TextAttack with a custom TensorFlow model.\n",
        "  \"\"\"\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  def __call__(self, text_input_list):\n",
        "    # retrieve model prediction\n",
        "    text_array = np.array(text_input_list)\n",
        "    tokens = tokenizer.texts_to_sequences(text_input_list) \n",
        "    tokens_pad = pad_sequences(tokens, maxlen=maxlen)\n",
        "    model_pred = self.model.predict(tokens_pad)\n",
        "\n",
        "    # return prediction scores as torch.Tensors\n",
        "    logits = torch.FloatTensor(model_pred)\n",
        "    logits = logits.squeeze(dim=-1)\n",
        "\n",
        "    # for each output, index 0 corresponds to the negative \n",
        "    # and index 1 corresponds to the positive confidence \n",
        "    final_preds = torch.stack((1-logits, logits), dim=1)\n",
        "\n",
        "    return final_preds"
      ],
      "metadata": {
        "id": "jZi_ze4UUPOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example output\n",
        "CustomTensorFlowModelWrapper(model)([\"this is negative text. bad terrible awful.\",\n",
        "                                     \"this is positive text. great amazing love\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk4zkQ-iUPQw",
        "outputId": "459498ba-3698-4a27-9331-786ea40e34f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9619, 0.0381],\n",
              "        [0.1963, 0.8037]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a successful text atack which fools the model into predicting the wrong label\n",
        "t1 = 'i love the tie dye and the accent stitching. back detail is fun!'\n",
        "t2 = 'i adore the tie colouring and the accent stitching. back detail is amusing!'\n",
        "CustomTensorFlowModelWrapper(model)([t1,t2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-1DDv91-MgD",
        "outputId": "1d1e05e8-02b9-4922-df6c-153012380042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1261, 0.8739],\n",
              "        [0.4111, 0.5889]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Attack"
      ],
      "metadata": {
        "id": "bA44EjTfKO9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model wrapper with the trained LSTM\n",
        "model_wrapper = CustomTensorFlowModelWrapper(model)\n",
        "\n",
        "# textattack requires custom datasets to be presented as a list of (input, ground-truth label) pairs\n",
        "data_pairs = []\n",
        "for input, label in zip(dataset['Review Text'], dataset['Recommended IND']):\n",
        "  data_pairs.append((input, label))\n",
        "\n",
        "new_dataset = Dataset(data_pairs, shuffle=True)"
      ],
      "metadata": {
        "id": "2DNEzVAPnwK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the four fundamental components of the attack\n",
        "from textattack.goal_functions.classification import UntargetedClassification\n",
        "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
        "from textattack.constraints.semantics import WordEmbeddingDistance\n",
        "from textattack.transformations import WordSwapEmbedding\n",
        "from textattack.search_methods import GreedyWordSwapWIR\n",
        "from textattack import Attack\n",
        "\n",
        "goal_function = UntargetedClassification(model_wrapper)\n",
        "\n",
        "constraints = [\n",
        "    RepeatModification(),\n",
        "    StopwordModification(),\n",
        "    WordEmbeddingDistance(min_cos_sim=0.9)\n",
        "]\n",
        "\n",
        "transformation = WordSwapEmbedding(max_candidates=50)\n",
        "\n",
        "search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
        "\n",
        "# construct the actual attack\n",
        "attack = Attack(goal_function, constraints, transformation, search_method)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHYNKa-XjHQg",
        "outputId": "cab5e469-85b9-44b7-b435-510877a31624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'keras.engine.sequential.Sequential'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.loggers import CSVLogger \n",
        "from textattack.attack_results import SuccessfulAttackResult\n",
        "from textattack import Attacker\n",
        "from textattack import AttackArgs\n",
        "from textattack.datasets import Dataset\n",
        "\n",
        "# attack until 1000 successfull attacks are reached\n",
        "attack_args = AttackArgs(num_successful_examples=1000, \n",
        "                         random_seed=random_state)\n",
        "\n",
        "attacker = Attacker(attack, new_dataset, attack_args)\n",
        "\n",
        "attack_results = attacker.attack_dataset()"
      ],
      "metadata": {
        "id": "dQwspHUrjHWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the attack results and the differences\n",
        "logger = CSVLogger(color_method='html')\n",
        "\n",
        "for result in attack_results:\n",
        "    logger.log_attack_result(result)\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(logger.df[['original_text', 'perturbed_text']].to_html(escape=False)))"
      ],
      "metadata": {
        "id": "-pKDAUzAjHZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QQD_RrhjjHcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}